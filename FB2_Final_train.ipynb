{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68111994-a60e-4aee-95cb-7bff2c54f7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3283d29981eb4ceabacce18236a6ea56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading text files #0:   0%|          | 0/2096 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f95cca29c1c4382972c5bbbdca2ff5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading text files #1:   0%|          | 0/2095 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2acf7785a3349aa81a27a0fc80c61fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/4191 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving dataset to disk: out\n",
      "====== Fold: 0 ======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForTokenClassification: ['mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'deberta.embeddings.position_embeddings.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using GPU: NVIDIA A40\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3353/3353 [15:09<00:00,  3.69it/s, Epoch=1, LR=2.79e-6, Train_Loss=0.714]\n",
      "100%|██████████| 838/838 [01:21<00:00, 10.30it/s, Epoch=1, LR=2.79e-6, Valid_Loss=0.624]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Improved (inf ---> 0.6239318072669172)\n",
      "Model Saved\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3353/3353 [15:06<00:00,  3.70it/s, Epoch=2, LR=2.79e-6, Train_Loss=0.599]\n",
      "100%|██████████| 838/838 [01:25<00:00,  9.85it/s, Epoch=2, LR=2.79e-6, Valid_Loss=0.605]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Improved (0.6239318072669172 ---> 0.6046331555567024)\n",
      "Model Saved\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3353/3353 [18:44<00:00,  2.98it/s, Epoch=3, LR=1e-6, Train_Loss=0.476]\n",
      "100%|██████████| 838/838 [01:30<00:00,  9.23it/s, Epoch=3, LR=1e-6, Valid_Loss=0.597]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Improved (0.6046331555567024 ---> 0.5972174988500552)\n",
      "Model Saved\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3353/3353 [22:19<00:00,  2.50it/s, Epoch=4, LR=1e-6, Train_Loss=0.417]\n",
      "100%|██████████| 838/838 [01:36<00:00,  8.65it/s, Epoch=4, LR=1e-6, Valid_Loss=0.615]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete in 1h 17m 41s\n",
      "Best Loss: 0.5972\n",
      "\n",
      "====== Fold: 1 ======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForTokenClassification: ['mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'deberta.embeddings.position_embeddings.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using GPU: NVIDIA A40\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3353/3353 [17:49<00:00,  3.13it/s, Epoch=1, LR=2.79e-6, Train_Loss=0.715]\n",
      "100%|██████████| 838/838 [01:25<00:00,  9.77it/s, Epoch=1, LR=2.79e-6, Valid_Loss=0.735]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Improved (inf ---> 0.7349020663064266)\n",
      "Model Saved\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3353/3353 [15:27<00:00,  3.61it/s, Epoch=2, LR=2.79e-6, Train_Loss=0.631]\n",
      "100%|██████████| 838/838 [01:28<00:00,  9.48it/s, Epoch=2, LR=2.79e-6, Valid_Loss=0.642]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Improved (0.7349020663064266 ---> 0.6415135208725694)\n",
      "Model Saved\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 3265/3353 [18:42<00:31,  2.77it/s, Epoch=3, LR=1e-6, Train_Loss=0.521]"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GroupKFold, KFold,StratifiedGroupKFold\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig, AdamW\n",
    "from transformers import DataCollatorWithPadding,DataCollatorForTokenClassification\n",
    "import warnings\n",
    "from sklearn.metrics import log_loss\n",
    "import torch.nn.functional as F\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from datasets import Dataset, load_from_disk\n",
    "import re\n",
    "from text_unidecode import unidecode\n",
    "from typing import Dict , List,Tuple\n",
    "import codecs\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "class CFG:\n",
    "    wandb = False\n",
    "    apex = True\n",
    "    model = 'microsoft/deberta-v3-large'\n",
    "    seed = 42\n",
    "    max_len = 1024\n",
    "    dropout = 0.2\n",
    "    target_size=3\n",
    "    n_accumulate=1\n",
    "    print_freq = 100\n",
    "    min_lr=1e-6\n",
    "    eps=1e-6\n",
    "    betas=(0.9, 0.999)\n",
    "    scheduler = 'CosineAnnealingLR'\n",
    "    batch_size = 1\n",
    "    num_workers = 8\n",
    "    lr = 1e-5\n",
    "    weight_decay = 0.01\n",
    "    epochs = 4\n",
    "    n_fold = 5\n",
    "    trn_fold=[i for i in range(n_fold)]\n",
    "    train = True \n",
    "    device = torch.device(\"cuda:0\" )\n",
    "    num_warmup_steps = 0\n",
    "    num_cycles=0.5\n",
    "    freezing = True\n",
    "    debug = True\n",
    "    T_max= 500,\n",
    "    debug_ver2 = False\n",
    "    gradient_checkpoint=False\n",
    "    load_from_disk= None\n",
    "OUTPUT_DIR = 'out'\n",
    "model_path = 'microsoft/deberta-v3-large'\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "INPUT_DIR = \"train\"\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "def get_logger(filename=OUTPUT_DIR+'train'):\n",
    "    from logging import getLogger, INFO, FileHandler, Formatter, StreamHandler\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = get_logger()\n",
    "\n",
    "def seed_everything(seed=CFG.seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(seed=42)    \n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "def replace_encoding_with_utf8(error:UnicodeError) -> Tuple[bytes,int]:\n",
    "    return error.object[error.start:error.end].encode('utf-8'),error.end\n",
    "def replace_decoding_with_cp1252(error:UnicodeError) -> Tuple[str,int]:\n",
    "    return error.object[error.start:error.end].decode('cp1252'),error.end\n",
    "codecs.register_error('replace_encoding_with_utf8',replace_encoding_with_utf8)\n",
    "codecs.register_error('replace_decoding_with_cp1252',replace_decoding_with_cp1252)\n",
    "def resolve_encodings_and_normalize(text:str) -> str:\n",
    "    text = (text.encode('raw_unicode_escape')\n",
    "            .decode('utf-8',errors = 'replace_decoding_with_cp1252')\n",
    "            .encode('cp1252',errors = 'replace_encoding_with_utf8')\n",
    "            .decode('utf-8',errors = 'replace_decoding_with_cp1252')\n",
    "           )\n",
    "    text = unidecode(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "def get_score(outputs, labels):\n",
    "    outputs = F.softmax(torch.tensor(outputs)).numpy()\n",
    "    score = log_loss(labels,outputs)\n",
    "    return round(score, 5)\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "def get_essay(essay_id, is_train=True):\n",
    "    parent_path = INPUT_DIR  if is_train else INPUT_DIR + 'test'\n",
    "    essay_path = os.path.join(parent_path, f\"{essay_id}.txt\")\n",
    "    essay_text = open(essay_path, 'r').read()\n",
    "    return essay_text\n",
    "\n",
    "def criterion_val(outputs, labels):\n",
    "    return nn.CrossEntropyLoss()(outputs, labels)\n",
    "def criterion_train(outputs, labels):\n",
    "    return nn.CrossEntropyLoss()(outputs, labels)\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "\n",
    "df['discourse_text'][293]='Cl' + df['discourse_text'][293]\n",
    "df['discourse_text'][790]='T' + df['discourse_text'][790]\n",
    "df['discourse_text'][879]='I' + df['discourse_text'][879]\n",
    "df['discourse_text'][2828]='w' + df['discourse_text'][2828]\n",
    "df['discourse_text'][4793]='i' + df['discourse_text'][4793]\n",
    "df['discourse_text'][8093]='I' + df['discourse_text'][8093]\n",
    "df['discourse_text'][9202]='l' + df['discourse_text'][9202]\n",
    "df['discourse_text'][9790]='I' + df['discourse_text'][9790]\n",
    "df['discourse_text'][14054]='i' + df['discourse_text'][14054]\n",
    "df['discourse_text'][14387]='s' + df['discourse_text'][14387]\n",
    "df['discourse_text'][15188]='i' + df['discourse_text'][15188]\n",
    "df['discourse_text'][15678]='I' + df['discourse_text'][15678]\n",
    "df['discourse_text'][16065]='f' + df['discourse_text'][16065]\n",
    "df['discourse_text'][16084]='I' + df['discourse_text'][16084]\n",
    "df['discourse_text'][16255]='T' + df['discourse_text'][16255]\n",
    "df['discourse_text'][17096]='I' + df['discourse_text'][17096]\n",
    "df['discourse_text'][17261]='t' + df['discourse_text'][17261]\n",
    "df['discourse_text'][18691]='I' + df['discourse_text'][18691]\n",
    "df['discourse_text'][19967]='t' + df['discourse_text'][19967]\n",
    "df['discourse_text'][20186]='b' + df['discourse_text'][20186]\n",
    "df['discourse_text'][20264]='I' + df['discourse_text'][20264]\n",
    "df['discourse_text'][20421]='i' + df['discourse_text'][20421]\n",
    "df['discourse_text'][20870]='h' + df['discourse_text'][20870]\n",
    "df['discourse_text'][22064]='t' + df['discourse_text'][22064]\n",
    "df['discourse_text'][22793]='I' + df['discourse_text'][22793]\n",
    "df['discourse_text'][22962]='W' + df['discourse_text'][22962]\n",
    "df['discourse_text'][23990]='f' + df['discourse_text'][23990]\n",
    "df['discourse_text'][24085]='w' + df['discourse_text'][24085]\n",
    "df['discourse_text'][25330]='a' + df['discourse_text'][25330]\n",
    "df['discourse_text'][25446]='i' + df['discourse_text'][25446]\n",
    "df['discourse_text'][25667]='S' + df['discourse_text'][25667]\n",
    "df['discourse_text'][25869]='I' + df['discourse_text'][25869]\n",
    "df['discourse_text'][26172]='i' + df['discourse_text'][26172]\n",
    "df['discourse_text'][26284]='I' + df['discourse_text'][26284]\n",
    "df['discourse_text'][26289]='t' + df['discourse_text'][26289]\n",
    "df['discourse_text'][26322]='t' + df['discourse_text'][26322]\n",
    "df['discourse_text'][26511]='t' + df['discourse_text'][26511]\n",
    "df['discourse_text'][27763]='I' + df['discourse_text'][27763]\n",
    "df['discourse_text'][28262]='P' + df['discourse_text'][28262]\n",
    "df['discourse_text'][29164]='bu' + df['discourse_text'][29164]\n",
    "df['discourse_text'][29519]='e' + df['discourse_text'][29519]\n",
    "df['discourse_text'][29532]='t' + df['discourse_text'][29532]\n",
    "df['discourse_text'][29571]='A' + df['discourse_text'][29571]\n",
    "df['discourse_text'][29621]='t' + df['discourse_text'][29621]\n",
    "df['discourse_text'][30791]='E' + df['discourse_text'][30791]\n",
    "df['discourse_text'][30799]='T' + df['discourse_text'][30799]\n",
    "df['discourse_text'][31519]='t' + df['discourse_text'][31519]\n",
    "df['discourse_text'][31597]='t' + df['discourse_text'][31597]\n",
    "df['discourse_text'][31992]='T' + df['discourse_text'][31992]\n",
    "df['discourse_text'][32086]='I' + df['discourse_text'][32086]\n",
    "df['discourse_text'][32204]='c' + df['discourse_text'][32204]\n",
    "df['discourse_text'][32341]='becaus' + df['discourse_text'][32341]\n",
    "df['discourse_text'][33246]='A' + df['discourse_text'][33246]\n",
    "df['discourse_text'][33819]='W' + df['discourse_text'][33819]\n",
    "df['discourse_text'][34023]='i' + df['discourse_text'][34023]\n",
    "df['discourse_text'][35467]='b' + df['discourse_text'][35467]\n",
    "df['discourse_text'][35902]='i' + df['discourse_text'][35902]  \n",
    "df['essay_text'] = df['essay_id'].apply(get_essay)\n",
    "df['essay_text'] = df['essay_text'].apply(lambda x : resolve_encodings_and_normalize(x))\n",
    "df['discourse_text'] = df['discourse_text'].apply(lambda x : resolve_encodings_and_normalize(x))\n",
    "    \n",
    "# 根据KFOLD划分数据\n",
    "# gkf = GroupKFold(n_splits=CONFIG['n_fold'])\n",
    "\n",
    "# for fold, (_, val_) in enumerate(gkf.split(X=df, groups=df.essay_id)):\n",
    "#     df.loc[val_, \"kfold\"] = int(fold)\n",
    "\n",
    "# df[\"kfold\"] = df[\"kfold\"].astype(int)\n",
    "# df.groupby('kfold')['discourse_effectiveness'].value_counts()\n",
    "\n",
    "# # 将 Ineffective Adequate Effective三个类别进行编码\n",
    "# encoder = LabelEncoder()\n",
    "# df['discourse_effectiveness'] = encoder.fit_transform(df['discourse_effectiveness'])\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "def replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n",
    "    return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n",
    "\n",
    "\n",
    "def replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n",
    "    return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "codecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\n",
    "codecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "def resolve_encodings_and_normalize(text: str) -> str:\n",
    "    \"\"\"Resolve the encoding problems and normalize the abnormal characters.\"\"\"\n",
    "    text = (\n",
    "        text.encode(\"raw_unicode_escape\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "        .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n",
    "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
    "    )\n",
    "    text = unidecode(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
    "    This is for REPRODUCIBILITY.'''\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "label2id = {\n",
    "    \"Adequate\": 0,\n",
    "    \"Effective\": 1,\n",
    "    \"Ineffective\": 2,\n",
    "}\n",
    "for i,item in enumerate(df['discourse_effectiveness']):\n",
    "      df['discourse_effectiveness'][i] = label2id[df['discourse_effectiveness'][i]]\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "# encoder = LabelEncoder()\n",
    "# df['discourse_effectiveness'] = encoder.fit_transform(df['discourse_effectiveness'])\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "def read_text_files(example, data_dir):\n",
    "\n",
    "    id_ = example[\"essay_id\"]\n",
    "\n",
    "    with open(data_dir / f\"{id_}.txt\", \"r\") as fp:\n",
    "        example[\"text\"] = resolve_encodings_and_normalize(fp.read())\n",
    "    return example\n",
    "\n",
    "set_seed(CFG.seed)\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "cfg={'load_from_disk':None} \n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "data_dir = Path('train')\n",
    "if cfg[\"load_from_disk\"]:\n",
    "    if not cfg[\"load_from_disk\"].endswith(\".dataset\"):\n",
    "        cfg[\"load_from_disk\"] += \".dataset\"\n",
    "    ds = load_from_disk(cfg['load_from_disk'])\n",
    "    \n",
    "    pkl_file = f\"{cfg['load_from_disk'][:-len('.dataset')]}_pkl\"\n",
    "    with open(pkl_file, \"rb\") as fp:\n",
    "        grouped = pickle.load(fp)\n",
    "    \n",
    "    \n",
    "    print(\"Loading from saved files\")\n",
    "else:\n",
    "    train_df = df\n",
    "\n",
    "    text_ds = Dataset.from_dict({\"essay_id\": df.essay_id.unique()})\n",
    "\n",
    "    text_ds = text_ds.map(\n",
    "        partial(read_text_files, data_dir=data_dir),\n",
    "        num_proc=2,\n",
    "        batched=False,\n",
    "        desc=\"Loading text files\",\n",
    "    )\n",
    "\n",
    "\n",
    "    text_df = text_ds.to_pandas()\n",
    "    \n",
    "    # train_df[\"discourse_text\"] = [\n",
    "    #     resolve_encodings_and_normalize(x) for x in train_df[\"discourse_text\"]\n",
    "    # ]\n",
    "\n",
    "    train_df = train_df.merge(text_df, on=\"essay_id\", how=\"left\")\n",
    "disc_types = [\n",
    "    \"Claim\",\n",
    "    \"Concluding Statement\",\n",
    "    \"Counterclaim\",\n",
    "    \"Evidence\",\n",
    "    \"Lead\",\n",
    "    \"Position\",\n",
    "    \"Rebuttal\",\n",
    "]\n",
    "cls_tokens_map = {label: f\"[CLS_{label.upper()}]\" for label in disc_types}\n",
    "end_tokens_map = {label: f\"[END_{label.upper()}]\" for label in disc_types}\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.add_special_tokens(\n",
    "    {\"additional_special_tokens\": list(cls_tokens_map.values())+list(end_tokens_map.values())}\n",
    ")\n",
    "cls_id_map = {\n",
    "    label: tokenizer.encode(tkn)[1]\n",
    "    for label, tkn in cls_tokens_map.items()\n",
    "}\n",
    "\n",
    "#targets = ds['discourse_effectiveness']\n",
    "#targets =  encoder.fit_transform(targets)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "# def find_positions(example):\n",
    "\n",
    "#     text = example[\"text\"][0]\n",
    "    \n",
    "#     # keeps track of what has already\n",
    "#     # been located\n",
    "#     min_idx = 0\n",
    "#     bad = []\n",
    "#     # stores start and end indexes of discourse_texts\n",
    "#     idxs = []\n",
    "#     c = []\n",
    "#     for dt in example[\"discourse_text\"]:\n",
    "#         # calling strip is essential\n",
    "#         matches = list(re.finditer(re.escape(dt.strip()), text))\n",
    "        \n",
    "#         # If there are multiple matches, take the first one\n",
    "#         # that is past the previous discourse texts.\n",
    "#         if len(matches) > 1:\n",
    "#             for m in matches:\n",
    "#                 if m.start() >= min_idx:\n",
    "#                     break\n",
    "#         # If no matches are found\n",
    "#         elif len(matches) == 0:\n",
    "#             idxs.append([0,10])\n",
    "#             continue  \n",
    "#         # If one match is found\n",
    "#         else:\n",
    "#             m = matches[0]\n",
    "            \n",
    "#         idxs.append([m.start(), m.end()])\n",
    "\n",
    "#         min_idx = m.start()\n",
    "#     return idxs\n",
    "    \n",
    "# def tokenize(example):\n",
    "#     example[\"idxs\"] = find_positions(example)\n",
    "\n",
    "#     text = example[\"text\"][0]\n",
    "#     chunks = []\n",
    "#     labels = []\n",
    "#     prev = 0\n",
    "\n",
    "#     zipped = zip(\n",
    "#         example[\"idxs\"],\n",
    "#         example[\"discourse_type\"],\n",
    "#         example[\"discourse_effectiveness\"],\n",
    "#     )\n",
    "#     for idxs, disc_type, disc_effect in zipped:\n",
    "#         # when the discourse_text wasn't found\n",
    "#         if idxs == [-1]:\n",
    "#             continue\n",
    "#         s, e = idxs\n",
    "\n",
    "#         # if the start of the current discourse_text is not \n",
    "#         # at the end of the previous one.\n",
    "#         # (text in between discourse_texts)\n",
    "#         if s != prev:\n",
    "#             chunks.append(text[prev:s])\n",
    "#             prev = s\n",
    "\n",
    "#         # if the start of the current discourse_text is \n",
    "#         # the same as the end of the previous discourse_text\n",
    "#         if s == prev:\n",
    "#             chunks.append(cls_tokens_map[disc_type])\n",
    "#             chunks.append(text[s:e])\n",
    "#             chunks.append(end_tokens_map[disc_type])\n",
    "        \n",
    "#         prev = e\n",
    "\n",
    "#         labels.append(label2id[disc_effect])\n",
    "\n",
    "#     tokenized = tokenizer(\n",
    "#         \" \".join(chunks),\n",
    "#         padding=False,\n",
    "#         truncation=True,\n",
    "#         max_length=CFG.max_len,\n",
    "#         add_special_tokens=True,\n",
    "#     )\n",
    "    \n",
    "#     # at this point, labels is not the same shape as input_ids.\n",
    "#     # The following loop will add -100 so that the loss function\n",
    "#     # ignores all tokens except CLS tokens\n",
    "\n",
    "#     # idx for labels list\n",
    "#     idx = 0\n",
    "#     final_labels = []\n",
    "#     for id_ in tokenized[\"input_ids\"]:\n",
    "#         # if this id belongs to a CLS token\n",
    "#         if id_ in cls_id_map.values():\n",
    "#             final_labels.append(1)\n",
    "#             idx += 1\n",
    "#         else:\n",
    "#             # -100 will be ignored by loss function\n",
    "#             final_labels.append(-100)\n",
    "    \n",
    "#     tokenized[\"labels\"] = final_labels\n",
    "\n",
    "#     return tokenized\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "\n",
    "def find_positions(example):\n",
    "\n",
    "    text = example[\"text\"][0]\n",
    "    \n",
    "    # keeps track of what has already\n",
    "    # been located\n",
    "    min_idx = 0\n",
    "    \n",
    "    # stores start and end indexes of discourse_texts\n",
    "    idxs = []\n",
    "    \n",
    "    for dt in example[\"discourse_text\"]:\n",
    "        # calling strip is essential\n",
    "        matches = list(re.finditer(re.escape(dt.strip()), text))\n",
    "        \n",
    "        # If there are multiple matches, take the first one\n",
    "        # that is past the previous discourse texts.\n",
    "        if len(matches) > 1:\n",
    "            for m in matches:\n",
    "                if m.start() >= min_idx:\n",
    "                    break\n",
    "        # If no matches are found\n",
    "        elif len(matches) == 0:\n",
    "            idxs.append([0,10]) # will filter out later\n",
    "            continue  \n",
    "        # If one match is found\n",
    "        else:\n",
    "            m = matches[0]\n",
    "            \n",
    "        idxs.append([m.start(), m.end()])\n",
    "\n",
    "        min_idx = m.start()\n",
    "\n",
    "    return idxs\n",
    "\n",
    "def tokenize(example):\n",
    "    example[\"idxs\"] = find_positions(example)\n",
    "\n",
    "    text = example[\"text\"][0]\n",
    "    chunks = []\n",
    "    labels = []\n",
    "    prev = 0\n",
    "\n",
    "    zipped = zip(\n",
    "        example[\"idxs\"],\n",
    "        example[\"discourse_type\"],\n",
    "        example[\"discourse_effectiveness\"],\n",
    "    )\n",
    "    for idxs, disc_type, disc_effect in zipped:\n",
    "        # when the discourse_text wasn't found\n",
    "        if idxs == [-1]:\n",
    "            continue\n",
    "\n",
    "        s, e = idxs\n",
    "       \n",
    "        # if the start of the current discourse_text is not \n",
    "        # at the end of the previous one.\n",
    "        # (text in between discourse_texts)\n",
    "        if s != prev:\n",
    "            chunks.append(text[prev:s])\n",
    "            prev = s\n",
    "\n",
    "        # if the start of the current discourse_text is \n",
    "        # the same as the end of the previous discourse_text\n",
    "        if s == prev:\n",
    "            chunks.append(cls_tokens_map[disc_type])\n",
    "            chunks.append(text[s:e])\n",
    "            chunks.append(end_tokens_map[disc_type])\n",
    "        \n",
    "        prev = e\n",
    "\n",
    "        #labels.append(label2id[disc_effect])\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        \" \".join(chunks),\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=CFG.max_len,\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "    \n",
    "    # at this point, labels is not the same shape as input_ids.\n",
    "    # The following loop will add -100 so that the loss function\n",
    "    # ignores all tokens except CLS tokens\n",
    "\n",
    "    # idx for labels list\n",
    "    idx = 0\n",
    "    final_labels = []\n",
    "    for id_ in tokenized[\"input_ids\"]:\n",
    "        # if this id belongs to a CLS token\n",
    "        if id_ in cls_id_map.values():\n",
    "            final_labels.append(1)\n",
    "            idx += 1\n",
    "        else:\n",
    "            # -100 will be ignored by loss function\n",
    "            final_labels.append(-100)\n",
    "    \n",
    "    tokenized[\"labels\"] = final_labels\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "\n",
    "#I frequently restart my notebook, so to reduce time\n",
    "# you can set this to just load the tokenized dataset from disk.\n",
    "# It gets loaded in the 3rd code cell, but a check is done here\n",
    "# to skip tokenizing\n",
    "if cfg[\"load_from_disk\"] is None:\n",
    "\n",
    "    # make lists of discourse_text, discourse_effectiveness\n",
    "    # for each essay\n",
    "    grouped = train_df.groupby([\"essay_id\"],sort = False).agg(list)\n",
    "    \n",
    "    ds = Dataset.from_pandas(grouped)\n",
    "    ds = ds.map(\n",
    "        tokenize,\n",
    "        batched=False,\n",
    "        num_proc=1,\n",
    "        desc=\"Tokenizing\",\n",
    "    )\n",
    "   \n",
    "    save_dir = f\"{OUTPUT_DIR}\"\n",
    "    #ds.to_csv('out.csv')\n",
    "    ds.save_to_disk(f\"{save_dir}.dataset\")\n",
    "    #ds.to_csv('out.csv')\n",
    "    with open(f\"{save_dir}_pkl\", \"wb\") as fp:\n",
    "        pickle.dump(grouped, fp)\n",
    "    print(\"Saving dataset to disk:\", OUTPUT_DIR)\n",
    "#encoder = LabelEncoder()\n",
    "#df['discourse_effectiveness'] = encoder.fit_transform(df['discourse_effectiveness'])\n",
    "\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "# gkf = GroupKFold(n_splits=cfg.n_fold)\n",
    "\n",
    "# for fold, (_, val_) in enumerate(gkf.split(X=df, groups=df.essay_id)):\n",
    "#     df.loc[val_, \"kfold\"] = int(fold)\n",
    "\n",
    "# df[\"kfold\"] = df[\"kfold\"].astype(int)\n",
    "# df.groupby('kfold')['discourse_effectiveness'].value_counts()\n",
    "\n",
    "# # 将 Ineffective Adequate Effective三个类别进行编码\n",
    "# encoder = LabelEncoder()\n",
    "# df['discourse_effectiveness'] = encoder.fit_transform(df['discourse_effectiveness'])\n",
    "\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "\n",
    "# def get_folds(ds, k_folds=5):\n",
    "    \n",
    "#     return [\n",
    "#         val_idx\n",
    "#         for _, val_idx in  enumerate(gkf.split(X=ds, groups=ds['labels']))\n",
    "#     ]\n",
    "\n",
    "# fold_idxs = get_folds(ds, 5)\n",
    "\n",
    "\n",
    "# In[26]:\n",
    "\n",
    "\n",
    "# #ds = pd.read_csv('out.csv')\n",
    "\n",
    "# gkf = GroupKFold(n_splits=5)\n",
    "# for fold, (_, val_) in enumerate(gkf.split(X=ds, groups=ds['labels'])):\n",
    "#     ds.loc[val_, \"kfold\"] = int(fold)\n",
    "\n",
    "# ds[\"kfold\"] = ds[\"kfold\"].astype(int)\n",
    "\n",
    "# #print(ds)\n",
    "\n",
    "\n",
    "# In[27]:\n",
    "\n",
    "\n",
    "\n",
    "# def prepare_loaders(fold):\n",
    "#     df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "#     df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "\n",
    "#     train_dataset = FeedBackDataset(df_train, tokenizer=CONFIG['tokenizer'], max_length=CONFIG['max_length'])\n",
    "#     valid_dataset = FeedBackDataset(df_valid, tokenizer=CONFIG['tokenizer'], max_length=CONFIG['max_length'])\n",
    "\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=CONFIG['train_batch_size'], collate_fn=collate_fn,\n",
    "#                               num_workers=8, shuffle=True, pin_memory=False, drop_last=True)\n",
    "#     valid_loader = DataLoader(valid_dataset, batch_size=CONFIG['valid_batch_size'], collate_fn=collate_fn,\n",
    "#                               num_workers=8, shuffle=False, pin_memory=False)\n",
    "\n",
    "#     return train_loader, valid_loader\n",
    "\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "\n",
    "# for fold in range(5):\n",
    "#     keep_cols = {\"input_ids\", \"attention_mask\", \"labels\",'discourse_effectiveness'}\n",
    "#     #train_idxs =  list(chain(*[i for f, i in enumerate(fold_idxs) if f != fold]))\n",
    "#     train_dataset = ds.drop([c for c in ds.columns if c not in keep_cols])\n",
    "#     eval_dataset = ds.drop([c for c in ds.columns if c not in keep_cols])\n",
    "# # train_dataset.to_csv('1.csv')\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "\n",
    "\n",
    "# def prepare_loaders(train_dataset,eval_dataset):\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=CFG.batch_size,collate_fn = collator,\n",
    "#                               num_workers=8, shuffle=True, pin_memory=False, drop_last=True)\n",
    "#     valid_loader = DataLoader(eval_dataset, batch_size=CFG.batch_size, collate_fn = collator,\n",
    "#                              num_workers=8, shuffle=False, pin_memory=False)\n",
    "\n",
    "#     return train_loader, valid_loader\n",
    "\n",
    "\n",
    "# In[30]:\n",
    "\n",
    "\n",
    "# for fold, (_, val_) in enumerate(gkf.split(X=df, groups=df.essay_id)):\n",
    "#     df.loc[val_, \"kfold\"] = int(fold)\n",
    "\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "\n",
    "# def get_folds(df, k_folds=5):\n",
    "\n",
    "#     sgkf = GroupKFold(n_splits=k_folds)\n",
    "#     return [\n",
    "#         val_idx\n",
    "#         for _, val_idx in sgkf.split(X=df, groups=df['essay_id'])\n",
    "#     ]\n",
    "\n",
    "# fold_idxs = get_folds(ds, 5)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[32]:\n",
    "\n",
    "\n",
    "# targets = data['labels'].to(device, dtype=torch.long)\n",
    "# batch_size = data['labels'][i].size(0)\n",
    "# drop = nn.Dropout(p=0.2)\n",
    "# output = model(data)\n",
    "#         pooler = WeightedLayerPooling(24,layer_start = 4,layer_weights=None)\n",
    "#         fc = nn.Linear(model_config.hidden_size, 3)\n",
    "#         fc.to(device)\n",
    "#         hidden_states  = output.hidden_states\n",
    "#         all_hidden_states = torch.stack(hidden_states).cuda()\n",
    "#         all_hidden_states.cuda()\n",
    "#         out = pooler(all_hidden_states)\n",
    "#         out = out[:, 0].cuda()\n",
    "#         out = drop(out).cuda()\n",
    "#         outputs = fc(out).cuda()\n",
    "#         #outputs(data)\n",
    "#         loss = criterion(outputs, targets)\n",
    "#         loss = loss / CONFIG['n_accumulate']\n",
    "#         loss.backward()\n",
    " \n",
    "\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "\n",
    "class WeightedLayerPooling(nn.Module):\n",
    "    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n",
    "        super(WeightedLayerPooling, self).__init__()\n",
    "        self.layer_start = layer_start\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.layer_weights = layer_weights if layer_weights is not None             else nn.Parameter(\n",
    "                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n",
    "            )\n",
    "\n",
    "    def forward(self, all_hidden_states):\n",
    "        all_layer_embedding = all_hidden_states[self.layer_start:, :, :, :]\n",
    "        all_layer_embedding.to(CFG.device)\n",
    "        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size()).to(CFG.device)\n",
    "        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n",
    "        weighted_average.to(CFG.device)\n",
    "        return weighted_average\n",
    "\n",
    "\n",
    "# In[34]:\n",
    "\n",
    "\n",
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "\n",
    "\n",
    "# In[35]:\n",
    "\n",
    "\n",
    "from transformers import Trainer, TrainingArguments, AutoConfig, AutoModelForTokenClassification, DataCollatorForTokenClassification,BertForTokenClassification,DebertaV2ForTokenClassification\n",
    "\n",
    "\n",
    "# In[36]:\n",
    "\n",
    "\n",
    "# collator = DataCollatorForTokenClassification(\n",
    "#     tokenizer=tokenizer, pad_to_multiple_of=16, padding=True\n",
    "# )\n",
    "\n",
    "# output = args.output_dir\n",
    "# for fold in range(CFG.n_fold):\n",
    "    \n",
    "#     args.output_dir = f\"{output}-fold{fold}\"\n",
    "    \n",
    "#     model_config = AutoConfig.from_pretrained(\n",
    "#             cfg[\"model_name_or_path\"],\n",
    "#         )\n",
    "#     model_config.update(\n",
    "#         {\n",
    "#             \"num_labels\": 3,\n",
    "#             \"cls_tokens\": list(cls_id_map.values()),\n",
    "#             \"label2id\": label2id,\n",
    "#             \"id2label\": {v:k for k, v in label2id.items()},\n",
    "#         }\n",
    "#     )\n",
    "    \n",
    "#     model = AutoModelForTokenClassification.from_pretrained(cfg[\"model_name_or_path\"], config=model_config)\n",
    "    \n",
    "#     # Because tokens were added, it is important to resize the embeddings\n",
    "#     model.resize_token_embeddings(len(tokenizer)) \n",
    "\n",
    "#     # split dataset to train and eval\n",
    "#     keep_cols = {\"input_ids\", \"attention_mask\", \"labels\"}\n",
    "#     train_idxs =  list(chain(*[i for f, i in enumerate(fold_idxs) if f != fold]))\n",
    "#     train_dataset = ds.select(train_idxs).remove_columns([c for c in ds.column_names if c not in keep_cols])\n",
    "#     eval_dataset = ds.select(fold_idxs[fold]).remove_columns([c for c in ds.column_names if c not in keep_cols])\n",
    "\n",
    "\n",
    "# In[37]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class FeedBackModel(nn.Module):\n",
    "    def __init__(self,model_name):\n",
    "        super(FeedBackModel, self).__init__()\n",
    "        self.model_config = AutoConfig.from_pretrained(\n",
    "            \"microsoft/deberta-v3-large\"\n",
    "        )\n",
    "        self.model_config.update(\n",
    "        {\n",
    "            #\"num_labels\": 3,\n",
    "            \"cls_tokens\": list(cls_id_map.values()),\n",
    "            #\"label2id\": label2id,\n",
    "            #\"id2label\": {v:k for k, v in label2id.items()},\n",
    "            'output_hidden_states':True,\n",
    "        }\n",
    "        )\n",
    "        self.mo = DebertaV2ForTokenClassification.from_pretrained(model_name,config = self.model_config)\n",
    "        # mo.to(CFG.device)\n",
    "        #self.model.resize_token_embeddings(len(tokenizer)) \n",
    "        self.drop = nn.Dropout(p=0.2)\n",
    "        #self.pooler = WeightedLayerPooling(24,layer_start = 4,layer_weights=None)\n",
    "        self.fc = nn.Linear(self.model_config.hidden_size, 3)\n",
    "        self.fc.to(CFG.device)\n",
    "    def forward(self,ids,mask):\n",
    "        output = self.mo(input_ids=ids,attention_mask=mask)\n",
    "        pooler = WeightedLayerPooling(24,layer_start = 4,layer_weights=None)\n",
    "        # fc = nn.Linear(self.model_config.hidden_size, 3)\n",
    "        hidden_states  = output.hidden_states\n",
    "        all_hidden_states = torch.stack(hidden_states).cuda()\n",
    "        all_hidden_states.cuda()\n",
    "        out = pooler(all_hidden_states)\n",
    "        #out = out[:, 0].cuda()\n",
    "        #out = self.drop(out).cuda()\n",
    "        #outputs = self.fc(out)\n",
    "        #hidden_states  = model(inputs).hidden_states\n",
    "        # hidden_1 = hidden_states[-1]\n",
    "        # hidden_2 = hidden_states[-2]\n",
    "        # hidden_3 = hidden_states[-3]\n",
    "        # hidden_4 = hidden_states[-4]\n",
    "        # all_ = torch.cat( hidden_1,hidden_2,hidden_3,hidden_4)\n",
    "        #out = out.view(-1, model_config.hidden_size)\n",
    "        outputs = self.fc(out)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[38]:\n",
    "\n",
    "\n",
    "# model = FeedBackModel(model_path)\n",
    "# model.train()\n",
    "# outputs = model\n",
    "\n",
    "\n",
    "# In[39]:\n",
    "\n",
    "\n",
    "# args={'do_train': True,\n",
    "#         \"do_eval\": True,\n",
    "#         \"per_device_train_batch_size\": 2,\n",
    "#         \"per_device_eval_batch_size\": 4,\n",
    "#         \"learning_rate\": 9e-6,\n",
    "#         \"weight_decay\": 0.01,\n",
    "#         \"num_train_epochs\": 3,\n",
    "#         \"warmup_ratio\": 0.1,\n",
    "#         \"optim\": 'adamw_torch',\n",
    "#         \"logging_steps\": 50,\n",
    "#         \"save_strategy\": \"epoch\",\n",
    "#         \"evaluation_strategy\": \"epoch\",\n",
    "#         \"report_to\": \"none\",\n",
    "#         \"group_by_length\": True,\n",
    "#         \"save_total_limit\": 1,\n",
    "#         \"metric_for_best_model\": \"loss\",\n",
    "#         \"greater_is_better\": False,\n",
    "#         \"seed\": 18}\n",
    "\n",
    "\n",
    "# In[40]:\n",
    "\n",
    "class AWP:\n",
    "    def __init__(\n",
    "            self,\n",
    "            model,\n",
    "            optimizer,\n",
    "            adv_param=\"weight\",\n",
    "            adv_lr=1,\n",
    "            adv_eps=0.2,\n",
    "            start_epoch=0,\n",
    "            adv_step=1,\n",
    "            scaler=None\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.adv_param = adv_param\n",
    "        self.adv_lr = adv_lr\n",
    "        self.adv_eps = adv_eps\n",
    "        self.start_epoch = start_epoch\n",
    "        self.adv_step = adv_step\n",
    "        self.backup = {}\n",
    "        self.backup_eps = {}\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def attack_backward(self, x, y, attention_mask, epoch):\n",
    "        if (self.adv_lr == 0) or (epoch < self.start_epoch):\n",
    "            return None\n",
    "\n",
    "        self._save()\n",
    "        for i in range(self.adv_step):\n",
    "            self._attack_step()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                adv_loss, tr_logits = self.model(input_ids=x, attention_mask=attention_mask, labels=y)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            self.scaler.scale(adv_loss).backward()\n",
    "\n",
    "        self._restore()\n",
    "\n",
    "    def _attack_step(self):\n",
    "        e = 1e-6\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
    "                norm1 = torch.norm(param.grad)\n",
    "                norm2 = torch.norm(param.data.detach())\n",
    "                if norm1 != 0 and not torch.isnan(norm1):\n",
    "                    r_at = self.adv_lr * param.grad / (norm1 + e) * (norm2 + e)\n",
    "                    param.data.add_(r_at)\n",
    "                    param.data = torch.min(\n",
    "                        torch.max(param.data, self.backup_eps[name][0]), self.backup_eps[name][1]\n",
    "                    )\n",
    "                # param.data.clamp_(*self.backup_eps[name])\n",
    "\n",
    "    def _save(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None and self.adv_param in name:\n",
    "                # 保存原始参数\n",
    "                if name not in self.backup:\n",
    "                    self.backup[name] = param.data.clone()\n",
    "                    grad_eps = self.adv_eps * param.abs().detach()\n",
    "                    self.backup_eps[name] = (\n",
    "                        self.backup[name] - grad_eps,\n",
    "                        self.backup[name] + grad_eps,\n",
    "                    )\n",
    "\n",
    "    def _restore(self, ):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if name in self.backup:\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n",
    "        self.backup_eps = {}\n",
    "\n",
    "\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments, AutoConfig, AutoModelForTokenClassification, DataCollatorForTokenClassification\n",
    "\n",
    "# args = args\n",
    "# # If using longformer, you will want to pad to a multiple of 512\n",
    "# # For most others, you'll want to pad to a multiple of 8\n",
    "# collator = DataCollatorForTokenClassification(\n",
    "#     tokenizer=tokenizer, pad_to_multiple_of=cfg[\"pad_multiple\"], padding=True\n",
    "# )\n",
    "\n",
    "# output = '/out/'\n",
    "# for fold in range(cfg[\"k_folds\"]):\n",
    "\n",
    "#     trainer = Trainer(\n",
    "#         model=model,\n",
    "#         args=args,\n",
    "#         train_dataset=train_dataset,\n",
    "#         eval_dataset=eval_dataset,\n",
    "#         tokenizer=tokenizer,\n",
    "#         data_collator=collator,\n",
    "#     )\n",
    "    \n",
    "#     trainer.train()\n",
    "#     optimizer = AdamW(model.parameters(), lr=CFG.lr, eps=CFG.eps, betas=CFG.betas)\n",
    "#     num_train_steps = int(len(train_data) / CFG.batch_size * CFG.epochs)\n",
    "#     scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n",
    "#     del model\n",
    "#     gc.collect()\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# In[41]:\n",
    "\n",
    "\n",
    "# trainer = Trainer(\n",
    "#         model=FeedBackModel,\n",
    "#         args=args,\n",
    "#         train_dataset=train_dataset,\n",
    "#         eval_dataset=eval_dataset,\n",
    "#         tokenizer=tokenizer,\n",
    "#         data_collator=collator,\n",
    "#     )\n",
    "\n",
    "\n",
    "# In[42]:\n",
    "\n",
    "\n",
    "# class FeedBackDataset(Dataset):\n",
    "#     def __init__(self, ds):\n",
    "#         self.ds = ds\n",
    "#         self.input_ids = ds['input_ids'].values\n",
    "#         self.attention_mask = ds['attention_mask'].values\n",
    "#         self.labels = ds['labels'].values\n",
    "#         self.discourse_effectiveness = ds['discourse_effectiveness'].values\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.ds)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         input_ids = self.input_ids[index]\n",
    "#         attention_mask = self.attention_mask[index]\n",
    "#         labels = self.labels[index]\n",
    "#         discourse_effectiveness = self.discourse_effectiveness[index]\n",
    "        \n",
    "#         return {\n",
    "#             'input_ids': input_ids,\n",
    "#             'attention_mask': attention_mask,\n",
    "#             'discourse_effectiveness':discourse_effectiveness,\n",
    "#             'labels' :labels\n",
    "#         }\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[43]:\n",
    "\n",
    "\n",
    "def criterion(outputs, labels):\n",
    "    return nn.CrossEntropyLoss()(outputs, labels)\n",
    "    #return nn.MSELoss()(outputs, labels)\n",
    "\n",
    "# 训练过程函数\n",
    "\n",
    "def train_one_epoch(swa_start,model, optimizer, scheduler, dataloader, device, epoch):\n",
    "    model.train()\n",
    "    swa_model = AveragedModel(model)\n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    pos = []\n",
    "    count = []\n",
    "    loss = 0.0\n",
    "    sum_loss = []\n",
    "    retain = []\n",
    "    a = 0\n",
    "    swa_scheduler =SWALR(optimizer,swa_lr=1e-6)\n",
    "    start_epoch = int(len(dataloader) / CFG.batch_size * CFG.epochs)\n",
    "    awp = AWP(model,\n",
    "              optimizer,\n",
    "              adv_lr=0.0000,\n",
    "              adv_eps=0.001,\n",
    "              start_epoch=start_epoch,\n",
    "              scaler=None\n",
    "              )\n",
    "    idx = 0\n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for step, data in bar:\n",
    "        idx += 1\n",
    "        ids = data['input_ids'].to(device, dtype=torch.long)\n",
    "        #ids = data['input_ids']\n",
    "        mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "        label  = data['labels'].to(device, dtype=torch.long)\n",
    "        targets = data['discourse_effectiveness'].to(device, dtype=torch.long)\n",
    "        for i,item in enumerate(label[0]):\n",
    "            if item.item() == 1:\n",
    "                pos.append(i)  \n",
    "                #count.append(i)\n",
    "            elif item.item() == -100:\n",
    "                 continue\n",
    "        #print(pos)\n",
    "        out = model(ids, mask)\n",
    "        #out = fc(out)\n",
    "        #sen_size = targets.size(0)\n",
    "        batch_size = ids.size(0)\n",
    "        #print(targets)\n",
    "        #print(out.size())\n",
    "        for i,item in enumerate(pos):\n",
    "            a = out[0][item]\n",
    "            a = a.unsqueeze(0)\n",
    "            #print(a.size())\n",
    "           # b = out[0][i+1]\n",
    "            tar = targets[0][i].unsqueeze(0)\n",
    "            #b=torch.unsqueeze(b,0)\n",
    "            lo =criterion(a,tar)\n",
    "            #loss = lo+retain\n",
    "            retain.append(lo)\n",
    "            #lo2=criterion(b, targets[0][int((2))].unsqueeze(0)) \n",
    "        #loss = (lo1+lo2)/2\n",
    "        loss = sum(retain)\n",
    "        loss = loss/len(pos)\n",
    "        pos = []\n",
    "        retain = []\n",
    "        #loss = loss / len(targets[0])\n",
    "        #print(loss)\n",
    "        loss.backward()\n",
    "        if loss < 0.66:\n",
    "            awp.attack_backward(ids, label, mask, idx)\n",
    "        if (step + 1) % CFG.n_accumulate == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            if epoch > swa_start:\n",
    "                swa_model.update_parameters(model)\n",
    "                swa_scheduler.step() \n",
    "            # zero the parameter gradients\n",
    "            else :\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        dataset_size += batch_size\n",
    "\n",
    "        epoch_loss = running_loss / dataset_size\n",
    "\n",
    "        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n",
    "                        LR=optimizer.param_groups[0]['lr'])\n",
    "    torch.optim.swa_utils.update_bn(train_loader, swa_model)\n",
    "    gc.collect()\n",
    "\n",
    "    return epoch_loss,swa_model\n",
    "\n",
    "\n",
    "# def train_one_epoch_swa(model, optimizer, scheduler, dataloader, device, epoch):\n",
    "#     model.train()\n",
    "#     dataset_size = 0\n",
    "#     running_loss = 0.0\n",
    "#     pos = []\n",
    "#     count = []\n",
    "#     loss = 0.0\n",
    "#     sum_loss = []\n",
    "#     retain = []\n",
    "#     a = 0\n",
    "#     start_epoch = int(len(dataloader) / CFG.batch_size * CFG.epochs)\n",
    "#     awp = AWP(model,\n",
    "#               optimizer,\n",
    "#               adv_lr=0.0000,\n",
    "#               adv_eps=0.001,\n",
    "#               start_epoch=start_epoch,\n",
    "#               scaler=None\n",
    "#               )\n",
    "#     idx = 0\n",
    "#     bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "#     for step, data in bar:\n",
    "#         idx += 1\n",
    "#         ids = data['input_ids'].to(device, dtype=torch.long)\n",
    "#         #ids = data['input_ids']\n",
    "#         mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "#         label  = data['labels'].to(device, dtype=torch.long)\n",
    "#         targets = data['discourse_effectiveness'].to(device, dtype=torch.long)\n",
    "#         for i,item in enumerate(label[0]):\n",
    "#             if item.item() == 1:\n",
    "#                 pos.append(i)\n",
    "#                 #count.append(i)\n",
    "#             elif item.item() == -100:\n",
    "#                  continue\n",
    "#         #print(pos)\n",
    "#         out = model(ids, mask)\n",
    "#         #out = fc(out)\n",
    "#         #sen_size = targets.size(0)\n",
    "#         batch_size = ids.size(0)\n",
    "#         #print(targets)\n",
    "#         #print(out.size())\n",
    "#         for i,item in enumerate(pos):\n",
    "#             a = out[0][item]\n",
    "#             a = a.unsqueeze(0)\n",
    "#             #print(a.size())\n",
    "#            # b = out[0][i+1]\n",
    "#             tar = targets[0][i].unsqueeze(0)\n",
    "#             #b=torch.unsqueeze(b,0)\n",
    "#             lo =criterion(a,tar)\n",
    "#             #loss = lo+retain\n",
    "#             retain.append(lo)\n",
    "#             #lo2=criterion(b, targets[0][int((2))].unsqueeze(0))\n",
    "#         #loss = (lo1+lo2)/2\n",
    "#         loss = sum(retain)\n",
    "#         loss = loss/len(pos)\n",
    "#         pos = []\n",
    "#         retain = []\n",
    "#         #loss = loss / len(targets[0])\n",
    "#         #print(loss)\n",
    "#         loss.backward()\n",
    "#         if loss < 0.7:\n",
    "#             awp.attack_backward(ids, label, mask, idx)\n",
    "#         if (step + 1) % CFG.n_accumulate == 0:\n",
    "#             optimizer.step()\n",
    "\n",
    "#             # zero the parameter gradients\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             if scheduler is not None:\n",
    "#                 scheduler.step()\n",
    "#         running_loss += (loss.item() * batch_size)\n",
    "#         dataset_size += batch_size\n",
    "\n",
    "#         epoch_loss = running_loss / dataset_size\n",
    "\n",
    "#         bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n",
    "#                         LR=optimizer.param_groups[0]['lr'])\n",
    "#     gc.collect()\n",
    "\n",
    "#     return epoch_loss\n",
    "\n",
    "\n",
    "# 验证过程函数\n",
    "# 验证过程函数\n",
    "\n",
    "@torch.no_grad()\n",
    "# def valid_one_epoch(swa_start,model, dataloader, device, epoch,swa_model):\n",
    "#     if epoch <= swa_start:\n",
    "#         model = model\n",
    "#     elif epoch> swa_start:\n",
    "#         model = swa_model\n",
    "#     model.eval()\n",
    "#     pos = []\n",
    "#     count = []\n",
    "#     dataset_size = 0\n",
    "#     running_loss = 0.0\n",
    "#     retain = []\n",
    "#     bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "#     for step, data in bar:\n",
    "#         ids = data['input_ids'].to(device, dtype=torch.long)\n",
    "#         mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "#         targets = data['discourse_effectiveness'].to(device, dtype=torch.long)\n",
    "#         label  = data['labels'].to(device, dtype=torch.float)\n",
    "#         for i,item in enumerate(label[0]):\n",
    "\n",
    "#             if item.item() == 1:\n",
    "#                 pos.append(i)\n",
    "#             elif item.item() == -100:\n",
    "#                  continue\n",
    "#         #print(pos) \n",
    "#         out = model(ids, mask)\n",
    "#         #out = fc(out)\n",
    "#         #sen_size = targets.size(0)\n",
    "#         batch_size = ids.size(0)    \n",
    "#         #print(targets)\n",
    "#         #print(out.size())\n",
    "#         for i,item in enumerate(pos):\n",
    "#             a = out[0][item]\n",
    "#             a= a.unsqueeze(0)\n",
    "#             #print(a.size())\n",
    "#            # b = out[0][i+1]\n",
    "#             tar = targets[0][i].unsqueeze(0)\n",
    "#             #b=torch.unsqueeze(b,0)\n",
    "#             lo =criterion(a,tar)\n",
    "#             retain.append(lo)\n",
    "            \n",
    "#             #lo2=criterion(b, targets[0][int((2))].unsqueeze(0)) \n",
    "#         #loss = (lo1+lo2)/2\n",
    "#         loss = sum(retain)\n",
    "#         loss = loss/len(pos)\n",
    "#         pos = []\n",
    "#         retain = []\n",
    "#             #loss_pre.append(loss_)\n",
    "#             #loss_pr = torch.tensor(loss_pre)\n",
    "#         # for i in range(len(data['discourse_effectiveness'][0])):\n",
    "#         #         ou = nn.Softmax(dim = 0)\n",
    "#         #         k = ou(out[0][i]).unsqueeze(0)\n",
    "#         #         a.append(k)    \n",
    "#         #loss = torch.mean(loss_pr)\n",
    "#         # targets = targets.view(-1)\n",
    "#         # # print(len(data['discourse_effectiveness'][0]))\n",
    "#         # loss = torch.tensor(loss, dtype=float)\n",
    "#         #outputs2 = model(ids,mask)\n",
    "\n",
    "#         #loss = criterion(outputs, targets.squeeze())\n",
    "\n",
    "#         running_loss += (loss.item() * batch_size)\n",
    "#         dataset_size += batch_size\n",
    "\n",
    "#         epoch_loss = running_loss / dataset_size\n",
    "\n",
    "#         bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss,\n",
    "#                         LR=optimizer.param_groups[0]['lr'])\n",
    "\n",
    "#     gc.collect()\n",
    "\n",
    "#     return epoch_loss\n",
    "\n",
    "from torch.optim.swa_utils import SWALR,AveragedModel\n",
    "\n",
    "def fetch_scheduler(optimizer):\n",
    "    if CFG.scheduler == 'CosineAnnealingLR':\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=500,\n",
    "                                                   eta_min=CFG.min_lr)\n",
    "    elif CFG.scheduler == 'CosineAnnealingWarmRestarts':\n",
    "        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=CFG.T_0,\n",
    "                                                             eta_min=CFG.min_lr)\n",
    "    elif CFG.scheduler == None:\n",
    "        return None\n",
    "\n",
    "    return scheduler\n",
    "\n",
    "# 开始训练函数\n",
    "def run_training(model, optimizer,device, num_epochs, fold):\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n",
    "\n",
    "    start = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_epoch_loss = np.inf\n",
    "    history = defaultdict(list)\n",
    "    #num_train_steps =int(len(train_loader)/CFG.batch_size*CFG.epochs)\n",
    "    #scheduler = get_scheduler(CFG.optimizer, num_train_steps)\n",
    "    swa_start = 2\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        gc.collect()\n",
    "#         if epoch == int(4):\n",
    "#             swa_train(fold)\n",
    "\n",
    "#         else:\n",
    "        scheduler = fetch_scheduler(optimizer)\n",
    "        train_epoch_loss,swa_model = train_one_epoch(swa_start,model, optimizer, scheduler,\n",
    "                                               dataloader=train_loader,\n",
    "                                               device=device, epoch=epoch)\n",
    "        history['Train Loss'].append(train_epoch_loss)\n",
    "\n",
    "        val_epoch_loss = valid_one_epoch(swa_start,model, valid_loader, device=device,\n",
    "                                         epoch=epoch,swa_model=swa_model)\n",
    "     \n",
    "        history['Valid Loss'].append(val_epoch_loss)\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        PATH = f\"/root/autodl-tmp/Loss-Fold-{fold}.bin\"\n",
    "        torch.save(model.state_dict(), PATH)\n",
    "        print(f\"Model Saved\")\n",
    "        # deep copy the model\n",
    "        # if val_epoch_loss <= best_epoch_loss:\n",
    "        #     print(f\"Validation Loss Improved ({best_epoch_loss} ---> {val_epoch_loss})\")\n",
    "        #     best_epoch_loss = val_epoch_loss\n",
    "        #     # run.summary[\"Best Loss\"] = best_epoch_loss\n",
    "        #     best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        #     PATH = f\"/root/autodl-tmp/Loss-Fold-{fold}.bin\"\n",
    "        #     torch.save(model.state_dict(), PATH)\n",
    "        #     print(f\"Model Saved\")\n",
    "\n",
    "        print()\n",
    "\n",
    "    end = time.time()\n",
    "    time_elapsed = end - start\n",
    "    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n",
    "    print(\"Best Loss: {:.4f}\".format(best_epoch_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model, history\n",
    "\n",
    "# 通过KFOLD加载数据集\n",
    "\n",
    "# def swa_train(fold):\n",
    "#     best_epoch_loss = np.inf\n",
    "#     num_epochs_swa = 1\n",
    "#     history = defaultdict(list)\n",
    "#     for epoch in range(1, num_epochs_swa + 1):\n",
    "#             model = FeedBackModel('microsoft/deberta-v3-large').to(CFG.device)# init your model class, build the graph shape\n",
    "#             state_dict = torch.load(f\"/root/autodl-tmp/Loss-Fold-{fold}.bin\")\n",
    "#             model.load_state_dict(state_dict)\n",
    "#             swa_model = AveragedModel(model).to(CFG.device)\n",
    "#             #model = AveragedModel(model,device=device)\n",
    "#             swa_scheduler = SWALR(optimizer, swa_lr=3e-6)\n",
    "#             epoch_loss = train_one_epoch_swa(swa_model, optimizer, swa_scheduler,\n",
    "#                                                dataloader=train_loader,\n",
    "#                                                device=CFG.device, epoch=epoch)\n",
    "#             history['Train Loss'].append(epoch_loss)\n",
    "#             swa_model.update_parameters(swa_model)\n",
    "#             swa_scheduler.step()\n",
    "#             torch.optim.swa_utils.update_bn(train_loader, swa_model, device=CFG.device)\n",
    "#             val_epoch_loss = valid_one_epoch(swa_model, valid_loader, device=CFG.device,\n",
    "#                                         epoch=epoch)       \n",
    "#             if val_epoch_loss <= best_epoch_loss:\n",
    "#                 print(f\"Validation Loss Improved ({best_epoch_loss} ---> {val_epoch_loss})\")\n",
    "#                 best_epoch_loss = val_epoch_loss\n",
    "#                 torch.save(swa_model.state_dict(), f\"/root/autodl-tmp/last-{fold}.pt\")\n",
    "def prepare_loaders(fold):\n",
    "    # train = ds[ds.kfold != fold].reset_index(drop=True)\n",
    "    # valid = ds[ds.kfold == fold].reset_index(drop=True)\n",
    "    keep_cols = {\"input_ids\", \"attention_mask\", \"labels\",'discourse_effectiveness'}\n",
    "    train_idxs =  list(chain(*[i for f, i in enumerate(fold_idxs) if f != fold]))    \n",
    "    collator = DataCollatorForTokenClassification(\n",
    "     tokenizer=tokenizer, pad_to_multiple_of=8, padding=True\n",
    " )\n",
    "    train_dataset = ds.remove_columns([c for c in ds.column_names if c not in keep_cols])\n",
    "    # eval_dataset = ds.select(fold_idxs[fold]).remove_columns([c for c in ds.column_names if c not in keep_cols])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CFG.batch_size,collate_fn = collator,\n",
    "                              num_workers=8, shuffle=True, pin_memory=False, drop_last=True)\n",
    "    # valid_loader = DataLoader(eval_dataset, batch_size=CFG.batch_size,collate_fn = collator,\n",
    "    #                           num_workers=8, shuffle=False, pin_memory=False)\n",
    "    # total_loader = DataLoader(ds,batch_size=CFG.batch_size,collate_fn = collator,\n",
    "    #                           num_workers=8, shuffle=False, pin_memory=False)\n",
    "    return train_loader\n",
    "\n",
    "\n",
    "# Run！\n",
    "\n",
    "for fold in range(0, 5):\n",
    "    print(f\"====== Fold: {fold} ======\")\n",
    "\n",
    "    # Create Dataloaders\n",
    "    train_loader, valid_loader ,total_loader= prepare_loaders(4)\n",
    "\n",
    "    Fmodel = FeedBackModel(model_path)\n",
    "    Fmodel.to(CFG.device)\n",
    "    ignored_params = list(map(id, Fmodel.fc.parameters()))\n",
    "    #awl = UncertaintyLoss(2)\n",
    "    base = filter(lambda p: id(p) not in ignored_params, Fmodel.parameters())\n",
    "    optimizer = AdamW([{'params': base}, {'params': Fmodel.fc.parameters(), 'lr': 3e-5}], lr=CFG.lr,weight_decay=CFG.weight_decay)\n",
    "\n",
    "    Fmodel, history = run_training(Fmodel, optimizer,\n",
    "                                  device=CFG.device,\n",
    "                                  num_epochs=CFG.epochs,\n",
    "                                  fold=fold)\n",
    "\n",
    "    del Fmodel, history, train_loader, valid_loader\n",
    "    _ = gc.collect()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b22d88-c319-47fe-81dc-aa5d6eaa56ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install text_unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465ac31e-ab5c-484b-bac8-0e9f74eef891",
   "metadata": {},
   "outputs": [],
   "source": [
    "swa_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924a7239-fddd-470a-9d53-478efce91d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_epoch_loss = np.inf\n",
    "# num_epochs_swa = 2\n",
    "# history = defaultdict(list)\n",
    "# for epoch in range(1, num_epochs_swa + 1):\n",
    "#             #state = torch.load(\"Loss-Fold-1.bin\")\n",
    "#             #model.load_state_dict(state)\n",
    "#             model = FeedBackModel('microsoft/deberta-v3-large').to(CFG.device)# init your model class, build the graph shape\n",
    "#             state_dict = torch.load(\"Loss-Fold-1.bin\")\n",
    "#             model.load_state_dict(state_dict)\n",
    "#             swa_model = AveragedModel(model).to(CFG.device)\n",
    "#             #model = AveragedModel(model,device=device)\n",
    "#             swa_scheduler = SWALR(optimizer, swa_lr=3e-6)\n",
    "#             epoch_loss = train_one_epoch_swa(swa_model, optimizer, swa_scheduler,\n",
    "#                                                dataloader=train_loader,\n",
    "#                                                device=CFG.device, epoch=epoch)\n",
    "#             history['Train Loss'].append(epoch_loss)\n",
    "#             swa_model.update_parameters(swa_model)\n",
    "#             swa_scheduler.step()\n",
    "#             torch.optim.swa_utils.update_bn(train_loader, swa_model, device=CFG.device)\n",
    "#             if epoch_loss <= best_epoch_loss:\n",
    "#                 print(f\"Validation Loss Improved ({best_epoch_loss} ---> {epoch_loss})\")\n",
    "#                 best_epoch_loss = epoch_loss\n",
    "#             # run.summary[\"Best Loss\"] = best_epoch_loss\n",
    "#                 torch.save(swa_model.state_dict(), \"last.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cbccbb-3a0f-4bb8-86ff-dc9b5222bf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "a = pd.read_csv('out.csv')\n",
    "a.to_csv('/root/autodl-tmp/1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1f6ae7-c757-4652-a023-06645b256b75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
